<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sparse Autoencoders (SAEs) find interpretable features in Stable Diffusion Turbo and enable fine-grained image editing.">
  <meta name="keywords" content="SAE, Sparse Autoencoder, Stable Diffusion, SDXL Turbo, mechanistic interpretability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unboxing SDXL Turbo with SAEs</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <!-- <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">

  <!-- Bulma Carousel CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel/dist/css/bulma-carousel.min.css">

  <!-- Bulma Slider CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">

  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/js/bulma-slider.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unboxing SDXL Turbo</h1>
          <h2 class="subtitle is-3 publication-subtitle"> How Sparse Autoencoders Unlock the Inner Workings of Text-to-Image Models</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ch.linkedin.com/in/vyacheslav-surkov" target="_blank">Viacheslav Surkov</a><sup>*,1</sup>,</span>
            <span class="author-block">
              <a href="https://wendlerc.github.io/" target="_blank">Chris Wendler</a><sup>*,1,2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/mari-antonio/" target="_blank">Antonio Mari</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://people.epfl.ch/mikhail.terekhov?lang=en" target="_blank">Mikhail Terekhov</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/justin.deschenaux/?lang=en" target="_blank">Justin Deschenaux</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/robert.west/?lang=en" target="_blank">Robert West</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.epfl.ch/caglar.gulcehre/?lang=en" target="_blank">Caglar Gulcehre</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://baulab.info" target="_blank">David Bau</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>equal contribution,</span>
            <span class="author-block"><sup>1</sup>EPFL, <sup>2</sup>Northeastern University</span>
          </div>

          <div class="column has-text-centered is-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.22366" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="fas fa-file-pdf"></i> -->
                       <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/surkovv/sdxl-unbox" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-brands fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/wendlerc/RIEBench" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa-brands fa-github"></i>
                  </span>
                  <span>RIEBench</span>
                  </a>
              </span>
              <!-- Colab Link. -->
              <span class="link-block">
                <a href="https://colab.research.google.com/drive/1lWZ2yCRwCf4iuykvb-91QYUNkuzIwI3k?usp=sharing"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-laptop-code"></i>
                  </span>
                  <span>Colab Demo</span>
                  </a>
                </span>
              
              <!-- HF spaces Link. -->
              <br>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/surokpro2/Unboxing_SDXL_with_SAEs"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>TurboBrush Demo</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/spaces/surokpro2/sdxl-sae-multistep"
                     target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-robot"></i>
                    </span>
                    <span>BaseBrush Demo</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://huggingface.co/spaces/surokpro2/sae_flux"
                       target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-robot"></i>
                      </span>
                      <span>FLUXBrush Demo</span>
                      </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small" style="margin-top: -80px;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/0 A sloth is eating dinner at his office desk.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/2301 A cartoon of a cat in a suit.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/3072 A dragon curled around a treasure hoard in a dark cave, depicted in a detailed fantasy oil painting style.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/3183 Strength 20 A bird sitting on a tree, 4K photography.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/4013 A portrait of a dog.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/4751 A cat wearing glasses, deep in thought, sitting at a computer desk covered in coding books and wires, depicted in a realistic photo style..webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/4998 A fox dressed in a suit, holding a briefcase and walking through a bustling city.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/up01 2615 A robotic dog running through a futuristic city, leaving a trail of sparks behind, illustrated in a retro-futuristic style.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/up01 90 Strength 5 A Viking warrior rowing a longship through icy waters, illustrated in a bold, graphic novel style..webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" autoplay muted loop playsinline height="100%">
            <source src="./static/clips_7s_bottom/up01 4977 Strength 5 A cat with a monocle sitting in a Victorian study, smoking a pipe, in a sepia-toned vintage photo style.webm"
                    type="video/webm">
          </video>
        </div>
      </div>
      <div class="subtitle has-text-centered">
        We decomposed the generative process of SDXL Turbo into interpretable features using Sparse Autoencoders. The features are causal and can be manipulated to control the generated images.
      </div>
    </div>
  </div>
</section>


<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-2">How it works</div>
        <p>
          Sparse Autoencoders (SAEs) are two-layer neural networks designed to reconstruct input vectors 
          while enforcing sparsity constraints in the hidden layer (i.e., only a few neurons are activated at once). 
          This sparsity enables SAEs to extract interpretable features, which have been shown to be useful in large 
          language models. Given this success, we explored the potential of SAEs 
          in the domain of text-to-image generative models.
        </p>
        <br>
        <p>
          We used SDXL Turbo, a model that integrates transformer-powered U-net architecture 
          to process input prompts for text-to-image generation. We selected four specific transformer 
          blocks within the U-net -- <i>down.2.1</i>, <i>mid.0</i>, <i>up.0.0</i>, and <i>up.0.1</i> -- 
          to analyze their roles in the generative process. Using a dataset of 1.5 million textual prompts from LAION-COCO, 
          we stored intermediate feature map representations from these blocks and trained four sparse autoencoders to 
          reconstruct these feature maps. The resulting learned features were not only interpretable but also 
          had a controllable causal effect on generated images. Manipulating specific features revealed distinct transformer 
          block specializations and enabled control over the generated images, such as adjusting semantic content 
          or enhancing visual details.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="has-text-centered title is-3">Where features are active</div>
    <p>
      For every feature, we collected images that fire the feature most strongly. Interestingly,
      the images reflect similar objects or concepts, like folders (down.2.1.#0), kitchen islands (down.2.1.#1), and so on.
      Note that the activation regions differ from quite scattered in the
      blocks <i>down.2.1</i> and <i>up.0.1</i> to more localized and concentrated on <i>mid.0</i> and <i>up.0.0</i>. 
    </p>
    <div class="columns">
      <div class="column is-full-width"> 
            <div class="sliders-container">
              <div class="top-image">
                <img src="./static/top/down_0.jpg" alt="Feature 1">
                <p>down.2.1 #0</p>
              </div>
              <div class="top-image">
                <img src="./static/top/down_1.jpg" alt="Feature 2">
                <p>down.2.1 #1</p>
              </div>
              <div class="top-image">
                <img src="./static/top/down_2.jpg" alt="Feature 3">
                <p>down.2.1 #2</p>
              </div>

              <div class="top-image">
                <img src="./static/top/mid_3.jpg" alt="Feature 1">
                <p>mid.0 #3</p>
              </div>
              <div class="top-image">
                <img src="./static/top/mid_50.jpg" alt="Feature 2">
                <p>mid.0 #50</p>
              </div>
              <div class="top-image">
                <img src="./static/top/mid_609.jpg" alt="Feature 3">
                <p>mid.0 #609</p>
              </div>
            
              <div class="top-image">
                <img src="./static/top/up_0.jpg" alt="Feature 1">
                <p>up.0.1 #0</p>
              </div>
              <div class="top-image">
                <img src="./static/top/up_1.jpg" alt="Feature 2">
                <p>up.0.1 #1</p>
              </div>
              <div class="top-image">
                <img src="./static/top/up_2.jpg" alt="Feature 3">
                <p>up.0.1 #2</p>
              </div>

              <div class="top-image">
                <img src="./static/top/up0_0.jpg" alt="Feature 1">
                <p>up.0.0 #0</p>
              </div>
              <div class="top-image">
                <img src="./static/top/up0_1.jpg" alt="Feature 2">
                <p>up.0.0 #1</p>
              </div>
              <div class="top-image">
                <img src="./static/top/up0_2.jpg" alt="Feature 3">
                <p>up.0.0 #2</p>
              </div>            
            </div>
      </div>
    </div>
    <div class="subtitle has-text-centered">
        These features are highly active during the image generation process. Notably, they appear to encode both concrete objects as well as abstract concepts.
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Features are causal</div>
        <p>
          Now, let's break into image generation process and try to manipulate feature values. You can increase the features' activation by using the sliders below and observe their effects.
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container">
          <div class="interpolation-container" data-folder="down_1802" data-frames="8">
              <h3 class="title is-4">down.2.1 #1802</h3>
              <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="down_89" data-frames="7">
            <h3 class="title is-4">down.2.1 #89</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="up0_4473" data-frames="7">
            <h3 class="title is-4">up.0.0 #4473</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="down_1678" data-frames="7">
            <h3 class="title is-4">down.2.1 #1678</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="up_4977" data-frames="7">
            <h3 class="title is-4">up.0.1 #4977</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="up0_4907" data-frames="7">
            <h3 class="title is-4">up.0.0 #4907</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="mid_4227" data-frames="7">
            <h3 class="title is-4">mid.0 #4227</h3>
            <!-- Slider will be dynamically added here -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Generalization to multi-step</div>
        <p>
          Interestingly, <b>our features</b> that we trained on 1-step SDXL Turbo <b>generalize to</b> 4-step SDXL Turbo and even to the multi-step <b>SDXL base model</b> without additional training.
          Here you can explore the effect of adding the features across different ranges of timesteps. Moving the slider changes the range of timesteps at which the intervention is performed in SDXL base. 
          We start from <b>[0, 25] and move the left side to [5, 25], [10, 25], [15, 25], [20, 25]</b>.
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container" id="multistep-containers">
          <script>
            // Configuration object for multistep folders
            const config = {
              multistepFolders: [
                'multistep_features_up_90_pirate',
                'multistep_features_up_90_cat',
                'multistep_features_up_4977_pirate',
                'multistep_features_up_4977_cat',
                'multistep_features_up_3718_pirate',
                'multistep_features_up_3718_cat',
                'multistep_features_up0_3742_pirate',
                'multistep_features_up0_3742_cat',
                'multistep_features_up0_1941_pirate',
                'multistep_features_up0_1941_cat',
                'multistep_features_down_4998_pirate',
                'multistep_features_down_4998_cat',
                'multistep_features_down_2301_pirate',
                'multistep_features_down_2301_cat'
              ]
            };

            // Function to create interpolation containers
            function createMultistepContainers() {
              const container = document.getElementById('multistep-containers');
              
              // Create container for each folder from config
              config.multistepFolders.forEach(folder => {
                const div = document.createElement('div');
                div.className = 'interpolation-container';
                div.dataset.folder = folder;
                div.dataset.frames = '5';

                const h3 = document.createElement('h3');
                h3.className = 'title is-4';
                h3.textContent = folder
                  .replace('multistep_features_up_', 'up.0.1 #')
                  .replace('multistep_features_up0_', 'up.0.0 #')
                  .replace('multistep_features_down_', 'down.2.1 #')
                  .replace('pirate', '')
                  .replace('cat', '')
                  .replace('_', '')
                
                div.appendChild(h3);
                div.appendChild(document.createComment(' Slider will be dynamically added here '));
                
                container.appendChild(div);
              });
            }

            // Call the function when document is ready
            document.addEventListener('DOMContentLoaded', createMultistepContainers);
          </script>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">FLUX Schnell</div>
        <p>
          We also trained a SAE on FLUX Schnell. Sliding to the right increases the intervention strength (= feature coefficient).
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container" id="flux-schnell-containers">
          <script>
            // Configuration object for multistep folders
            const configfluxschnell = {
              fluxschnellfolders: [
                'flux_schnell_cat',
                'flux_schnell_comic',
                'flux_schnell_evil',
                'flux_schnell_fire',
                'flux_schnell_manga',
                'flux_schnell_sand'
              ]
            };

            // Function to create interpolation containers
            function createMultistepContainers() {
              const container = document.getElementById('flux-schnell-containers');
              
              // Create container for each folder from config
              configfluxschnell.fluxschnellfolders.forEach(folder => {
                const div = document.createElement('div');
                div.className = 'interpolation-container';
                div.dataset.folder = folder;
                div.dataset.frames = '6';

                const h3 = document.createElement('h3');
                h3.className = 'title is-4';
                h3.textContent = folder
                  .replace('flux_schnell_', '')
                
                div.appendChild(h3);
                div.appendChild(document.createComment(' Slider will be dynamically added here '));
                
                container.appendChild(div);
              });
            }

            // Call the function when document is ready
            document.addEventListener('DOMContentLoaded', createMultistepContainers);
          </script>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">FLUX dev</div>
        <p>
          Our FLUX Schnell SAE also transfers to FLUX dev. Sliding to the right increases the intervention strength (= feature coefficient).
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container" id="flux-dev-containers">
          <script>
            // Configuration object for multistep folders
            const configfluxdev = {
              fluxdevfolders: [
                'flux_dev_cat',
                'flux_dev_comic',
                'flux_dev_evil',
                'flux_dev_fire',
                'flux_dev_manga',
                'flux_dev_sand'
              ]
            };

            // Function to create interpolation containers
            function createMultistepContainers() {
              const container = document.getElementById('flux-dev-containers');
              
              // Create container for each folder from config
              configfluxdev.fluxdevfolders.forEach(folder => {
                const div = document.createElement('div');
                div.className = 'interpolation-container';
                div.dataset.folder = folder;
                div.dataset.frames = '6';

                const h3 = document.createElement('h3');
                h3.className = 'title is-4';
                h3.textContent = folder
                  .replace('flux_dev_', '')
                
                div.appendChild(h3);
                div.appendChild(document.createComment(' Slider will be dynamically added here '));
                
                container.appendChild(div);
              });
            }

            // Call the function when document is ready
            document.addEventListener('DOMContentLoaded', createMultistepContainers);
          </script>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">RIEBench</div>
        <p>
          We introduce a new representation-based image editing benchmark, RIEBench. The key idea is to transfer features between parallell forward passes as shown below. 
          Dragging the slider corresponds to varying number of features transferred across within the indicated blocks. We precomputed results for a <b>SAE</b> with k=10 and expansion factor 4 transporting <b>1, 2, 4, 8, 16, 32, 64</b> features.
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container" id="animation-containers">
          <script>
            // Configuration object for multistep folders
            const configanimation = {
              folders: [
                'animation_10_4_down',
                'animation_10_4_up01',
                'animation_10_4_up00',
                'animation_10_4_mid',
                'animation_10_4_all'
              ]
            };

            // Function to create interpolation containers
            function createMultistepContainers() {
              const container = document.getElementById('animation-containers');
              
              // Create container for each folder from config
              configanimation.folders.forEach(folder => {
                const div = document.createElement('div');
                div.className = 'interpolation-container-wide';
                div.dataset.folder = folder;
                div.dataset.frames = '7';

                const h3 = document.createElement('h3');
                h3.className = 'title is-4';
                h3.textContent = folder
                  .replace('animation_10_4_', '')
                  .replace('down','down.2.1')
                  .replace('up00','up.0.0')
                  .replace('up01','up.0.1')
                  .replace('mid','mid.0')
                
                div.appendChild(h3);
                div.appendChild(document.createComment(' Slider will be dynamically added here '));
                
                container.appendChild(div);
              });
            }

            // Call the function when document is ready
            document.addEventListener('DOMContentLoaded', createMultistepContainers);
          </script>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <p>
          The same intervention can be performed for <b>neurons</b>. 
          Dragging the slider corresponds to varying number of features transferred across within the indicated blocks. We precomputed results for transporting <b>1000, 2000, 4000, 8000, 160000, 32000, 64000</b> neurons.
          For more information check out our <a href="https://github.com/wendlerc/RIEBench" target="_blank">RIEBench repository</a>.
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container" id="neuron-animation-containers">
          <script>
            // Configuration object for multistep folders
            const configneuronanimation = {
              folders: [
                'animation_neurons_10_4_down',
                'animation_neurons_10_4_up01',
                'animation_neurons_10_4_up00',
                'animation_neurons_10_4_mid',
                'animation_neurons_10_4_all'
              ]
            };

            // Function to create interpolation containers
            function createMultistepContainers() {
              const container = document.getElementById('neuron-animation-containers');
              
              // Create container for each folder from config
              configneuronanimation.folders.forEach(folder => {
                const div = document.createElement('div');
                div.className = 'interpolation-container-wide';
                div.dataset.folder = folder;
                div.dataset.frames = '7';

                const h3 = document.createElement('h3');
                h3.className = 'title is-4';
                h3.textContent = folder
                  .replace('animation_neurons_10_4_', '')
                  .replace('down','down.2.1')
                  .replace('up00','up.0.0')
                  .replace('up01','up.0.1')
                  .replace('mid','mid.0')
                
                div.appendChild(h3);
                div.appendChild(document.createComment(' Slider will be dynamically added here '));
                
                container.appendChild(div);
              });
            }

            // Call the function when document is ready
            document.addEventListener('DOMContentLoaded', createMultistepContainers);
          </script>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Single feature generates meaningful image</div>
        <p>Up until now, we discovered how features affect images generated with a meaningful prompt. However, would the features 
          make sense independently of any textual description? To investigate it, we tried to generate an image given an empty prompt, and turning off
          all the features but one. That's what happened:
        </p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="sliders-container">
          <div class="zero-image">
            <img src="./static/zeroint/down.jpg" alt="Feature 1">
            <p>down.2.1</p>
          </div>
          <div class="zero-image">
            <img src="./static/zeroint/up.jpg" alt="Feature 2">
            <p>up.0.1</p>
          </div>           
        </div>
        <p>
            down.2.1 and up.0.1 features generate meaningful images even than being turned on during empty-prompt generation. down.2.1
            interventions result in meaningful images, whereas up.0.1 interventions generate texture-like images.
        
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="has-text-centered title is-3">Empty-prompt generations as previews</div>
      <div id="results-carousel-2" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/0 A sloth is eating dinner at his office desk.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/2301 A cartoon of a cat in a suit.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/3072 A dragon curled around a treasure hoard in a dark cave, depicted in a detailed fantasy oil painting style.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/3183 Strength 20 A bird sitting on a tree, 4K photography.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/4013 A portrait of a dog.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/4751 A cat wearing glasses, deep in thought, sitting at a computer desk covered in coding books and wires, depicted in a realistic photo style..webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/4998 A fox dressed in a suit, holding a briefcase and walking through a bustling city.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/up01 2615 A robotic dog running through a futuristic city, leaving a trail of sparks behind, illustrated in a retro-futuristic style.webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/up01 90 Strength 5 A Viking warrior rowing a longship through icy waters, illustrated in a bold, graphic novel style..webm"
                    type="video/webm">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" controls muted loop playsinline height="100%">
            <source src="./static/clips_7s/up01 4977 Strength 5 A cat with a monocle sitting in a Victorian study, smoking a pipe, in a sepia-toned vintage photo style.webm"
                    type="video/webm">
          </video>
        </div>
      </div>
      <!--<div class="subtitle has-text-centered">-->
      <p>
        The empty-prompt generations can be used as a visual representation or preview of the features. Here is the brush animation from the beginning with the corresponding preview images. 
        <a href="https://www.goodfire.ai/blog/painting-with-concepts" target="_blank">Goodfire</a> in their recent blog post used our technique in their 
        <a href="https://paint.goodfire.ai/umap.html" target="_blank">UMAP</a> visualization, where our zero-shot intervention previews appear when hovering over the points.
      </p>
      <!--</div>-->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Try it yourself!</div>
        <p>
          We trained SAEs on SDXL Turbo's intermediate representations and showed that the learned features are interpretable and causal. 
          Moreover, they demonstrate the blocks in the SDXL Turbo's have different roles in the image generation process.
          Our hypothesis is:
          <br>
          - <i>down.2.1</i> defines what will be shown on the images (compositional block)
          <br>
          - <i>mid.0</i> assigns some low-level abstract semantics
          <br>
          - <i>up.0.0</i> adds details (details block)
          <br>
          - <i>up.0.1</i> generates colors and textures (style block)
        </p>
        <p>
          Now it is your turn! Try our demo application and explore 20K+ features. 
        </p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <div class="publication-links">
          <span class="link-block">
            <a href="https://huggingface.co/spaces/surokpro2/Unboxing_SDXL_with_SAEs"
               target="_blank"
               class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <i class="fas fa-robot"></i>
              </span>
              <span>TurboBrush Demo</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://huggingface.co/spaces/surokpro2/sdxl-sae-multistep"
                 target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-robot"></i>
                </span>
                <span>BaseBrush Demo</span>
                </a>
            </span>
            <span class="link-block">
                <a href="https://huggingface.co/spaces/surokpro2/sae_flux"
                   target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>FLUXBrush Demo</span>
                  </a>
            </span>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Image Interpolation</h2>
    
    <div class="interpolation-container" data-folder="stacked" data-frames="10">
      <h3 class="title is-4">Interpolation 1</h3>
      <!- - Slider will be dynamically added here - ->
    </div>

    <div class="interpolation-container" data-folder="stacked2" data-frames="10">
      <h3 class="title is-4">Interpolation 2</h3>
      <!- - Slider will be dynamically added here - ->
    </div>

    <!- - Add more interpolation-container divs as needed - ->

  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>The first version of our paper, which is <b>the first work leveraging SAEs within text-to-image diffusion models</b>.</p>
    <pre><code>@misc{surkov2024unpackingsdxlturbointerpreting,
      title={Unpacking SDXL Turbo: Interpreting Text-to-Image Models with Sparse Autoencoders}, 
      author={Viacheslav Surkov and Chris Wendler and Mikhail Terekhov and Justin Deschenaux and Robert West and Caglar Gulcehre},
      year={2024},
      eprint={2410.22366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.22366}, 
}
</code></pre>
<p>Updated version of the paper, showcasing multi-step generalization and a few more nice findings that we made along the way.</p>
    <pre><code>@misc{surkov2025onestepenoughsparseautoencoders,
      title={One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models}, 
      author={Viacheslav Surkov and Chris Wendler and Antonio Mari and Mikhail Terekhov and Justin Deschenaux and Robert West and Caglar Gulcehre and David Bau},
      year={2025},
      eprint={2410.22366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.22366}, 
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
        <p>
          Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
          href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
