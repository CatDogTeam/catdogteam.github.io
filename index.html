<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sparse Autoencoders (SAEs) find interpretable features in Stable Diffusion Turbo and enable fine-grained image editing.">
  <meta name="keywords" content="SAE, Sparse Autoencoder, Stable Diffusion, SDXL Turbo, mechanistic interpretability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Unboxing SDXL Turbo with SAEs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <!-- <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.3/css/bulma.min.css">

  <!-- Bulma Carousel CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel/dist/css/bulma-carousel.min.css">

  <!-- Bulma Slider CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">

  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  
  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/js/bulma-carousel.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/js/bulma-slider.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unboxing SDXL Turbo</h1>
          <h2 class="subtitle is-3 publication-subtitle"> How Sparse Autoencoders Unlock the Inner Workings of Text-to-Image Models</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Viacheslav Surkov</a>,</span>
            <span class="author-block">
              <a href="">Chris Wendler</a>,</span>
            <span class="author-block">
              <a href="">Mikhail Terekhov</a>,
            </span>
            <span class="author-block">
              <a href="">Justin Deschenaux</a>,
            </span>
            <span class="author-block">
              <a href="">Robert West</a>,
            </span>
            <span class="author-block">
              <a href="">Caglar Gulcehre
              </a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">EPFL</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper TODO</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv TODO</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video TODO</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code TODO</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data TODO</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp"
                type="video/mp4" placeholder="A video of horisontally stacked 4 videos of feature painting, one per each layer">
      </video> -->
      <div class="red-rect-placeholder"></div>
      <h4 class="has-text-centered slava-comment">
        SLAVA: Here I want to have a short teaser video(s) that shows editing of images with a brush (with young feature, evil feature, cartoon/anime feature). 
      </h4>
      <div class="subtitle has-text-centered">
      We trained SAEs on updates of 4 SDXL Turbo's transformer blocks. We find features to be causal: their manipulations enable image editing.
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Sparse autoencoders (SAEs) have emerged as a promising approach to reverse engineering large language models (LLMs). For LLMs, they have been shown to
          decompose intermediate representations that often are not interpretable directly
          into sparse sums of interpretable features, facilitating better control and subsequent analysis. Thus far, similar approaches have remained under-explored for
          text-to-image models. </p> <p> We investigated the possibility of using SAEs to learn interpretable features for few-step text-to-image diffusion models such as SDXL
          Turbo. To this end, we trained SAEs on the updates performed by transformer
          blocks within SDXL Turbo's denoising U-net. </p> <p> We find that their learned features
          are interpretable, causally influence the generation process, and reveal specialization among the blocks. In particular, we find blocks specializing in image
          composition, adding local details, color, illumination, and style. Our work thus
          represents an encouraging first step towards better understanding the internals of
          generative text-to-image models.</p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video.
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section hero-body">
  <div class="container is-max-desktop">
    <div class="red-rect-placeholder"></div>
    <h4 class="has-text-centered slava-comment">
      SLAVA: Here are 4 visualizations for sloth in tuxedo prompt with heatmaps. User can select a feature from radio bar to the right of each visualization.
    </h4>
    <div class="subtitle has-text-centered">
      Activation regions of the learned SAE features.   
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-2">How it works</div>
        <p>
          Sparse Autoencoders (SAEs) are two-layer neural networks designed to reconstruct input vectors 
          while enforcing sparsity constraints in the hidden layer (i.e., only a few neurons are activated at once). 
          This sparsity enables SAEs to extract interpretable features, which have been shown to be useful in large 
          language models. Given this success, we explored the potential of SAEs 
          in the domain of text-to-image generative models.
        </p>
        <br>
        <p>
          We used SDXL Turbo, a model that integrates transformer-powered U-net architecture 
          to process input prompts for text-to-image generation. We selected four specific transformer 
          blocks within the U-net -- <i>down.2.1</i>, <i>mid.0</i>, <i>up.0.0</i>, and <i>up.0.1</i> -- 
          to analyze their roles in the generative process. Using a dataset of 1.5 million textual prompts from LAION-COCO, 
          we stored intermediate feature map representations from these blocks and trained four sparse autoencoders to 
          reconstruct these feature maps. The resulting learned features were not only interpretable but also 
          had a controllable causal effect on generated images. Manipulating specific features revealed distinct transformer 
          block specializations and enabled control over the generated images, such as adjusting semantic content 
          or enhancing visual details.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Where features are active</div>
        <p>
          For every feature, we collected images that fire the feature most strongly. Interestingly,
          the images reflect similar objects or concepts, like... .
          Note that the activation regions differ from quite scattered in the
          blocks <i>down.2.1</i> and <i>up.0.1</i> to more localized and concentrated on <i>mid.0</i> and <i>up.0.0</i>. 
        </p>
        <div class="red-rect-placeholder"></div>
        <div class="slava-comment">SLAVA: Some features examples with images of highest activation in the same format as above</div>
        <div class="subtitle has-text-centered">
            These features are very active during generation process of images above. Noticably, they seem to encode concrete objects
            as well as more abstract concepts.
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Features are causal</div>
        <p>
          Now, let's break into image generation process and try to manipulate feature values.
          arc
          The interventions are indeed effective 
        </p>
      </div>
    </div>
    <div class="columns">
      <div class="column is-full-width"> 
        <div class="sliders-container">
          <div class="interpolation-container" data-folder="down_1802" data-frames="8">
              <h3 class="title is-4">down.2.1 #1802</h3>
              <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="down_89" data-frames="7">
            <h3 class="title is-4">down.2.1 #89</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="up0_4473" data-frames="7">
            <h3 class="title is-4">up.0.0 #4473</h3>
            <!-- Slider will be dynamically added here -->
          </div>
          <div class="interpolation-container" data-folder="z" data-frames="8">
            <h3 class="title is-4">mid.0 #228</h3>
            <!-- Slider will be dynamically added here -->
          </div>
        </div>
        <div class="slava-comment">SLAVA: Carousel with sliders. We should include examples of all 4 layers</div>
        
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <p>Up until now, we discovered how does features affect images generated with a meaningful prompt. However, would the features 
          make sense independently of the prompt? To investigate it, we tried to generate an image given an empty prompt, and turning off
          all the features but one. That's what happened:
        </p>
        <div class="red-rect-placeholder"></div>
        <div class="slava-comment">SLAVA: Examples of empty-prompt interventions of down.2.1 and up.0.1</div>
        <div class="subtitle has-text-centered">
            down.2.1 and up.0.1 features generate meaningful images even than being turned on during empty-prompt generation. down.2.1
            interventions result in meaningful images, whereas up.0.1 interventions generate texture-like images.
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width"> 
        <div class="has-text-centered title is-3">Try it yourself!</div>
        <p>
          We trained SAEs on SDXL Turbo's intermediate representations and showed that learned features are interpretable. 
          Moreover, they demonstrate the blocks in the SDXL Turbo's have different roles in the image generation process.
          Our hypothesis is:
          <br>
          - <i>down.2.1</i> defines what will be shown on the images (compositional block)
          <br>
          - <i>mid.0</i> assigns some low-level abstract semantics
          <br>
          - <i>up.0.0</i> adds details (details block)
          <br>
          - <i>up.0.1</i> generates colors and textures (style block)
        </p>
        <p>
          Now it is your turn! Try our demo application and explore 20K+ features. 
        </p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <div class="publication-links">
          <span class="link-block">
            <a href="https://arxiv.org/abs/2011.12948"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <!-- <i class="ai ai-arxiv"></i> -->
              </span>
              <span>Gradio+HuggingFace TODO</span>
            </a>
          </span>
          <!-- Video Link. -->
          <span class="link-block">
            <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                  <!-- <i class="fab fa-youtube"></i> -->
              </span>
            <span>Google Colab TODO</span>
          </a>
        </span>
      </div>
    </div>
  </div>
</section>

<section class="section is-light">
  <div class="container is-max-desktop">
    <h3 class="title is-2">Legacy</h3>
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <!-- Interpolating.
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
       Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Image Interpolation</h2>
    
    <div class="interpolation-container" data-folder="stacked" data-frames="10">
      <h3 class="title is-4">Interpolation 1</h3>
      <!- - Slider will be dynamically added here - ->
    </div>

    <div class="interpolation-container" data-folder="stacked2" data-frames="10">
      <h3 class="title is-4">Interpolation 2</h3>
      <!- - Slider will be dynamically added here - ->
    </div>

    <!- - Add more interpolation-container divs as needed - ->

  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>